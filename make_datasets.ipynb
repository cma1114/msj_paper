{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from initialize import *\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds_or = load_dataset(\"bench-llm/or-bench\", \"or-bench-80k\")\n",
    "ds_or_hard = load_dataset(\"bench-llm/or-bench\", \"or-bench-hard-1k\")\n",
    "ds_or_toxic = load_dataset(\"bench-llm/or-bench\", \"or-bench-toxic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_path = \"cackerman/ft_0to31_interleaved_stdmixsafecombonarafa_none_mult0\"#ft_randalias_0to31_interleaved_stdmixsafecombo6_none_mult0\"\n",
    "#model = load_model(model_path, base_model_path, bnb = False)\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "model.eval()\n",
    "sampling_kwargs = {\"use_cache\": True, \"pad_token_id\": model.tokenizer.eos_token_id, \"max_new_tokens\": 120, \"do_sample\": False, \"top_p\": None, \"temperature\": None}\n",
    "seed = 42\n",
    "N=10\n",
    "df = ds_or_hard['train'].to_pandas()\n",
    "sampled_df = df.groupby('category', group_keys=False).apply(lambda x: x.sample(n=N, random_state=seed))\n",
    "categories = sampled_df['category'].tolist()\n",
    "prompts = sampled_df['prompt'].tolist()\n",
    "prompts = [f\"<|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" for prompt in prompts]\n",
    "#for category,prompt in zip(categories,prompts): print(f\"{category}: {prompt}\")\n",
    "batch_size=8\n",
    "generated_responses=[]\n",
    "for batch_start in range(0, len(prompts), batch_size):\n",
    "    print(f\"Starting {batch_start}\")\n",
    "    batch_start_time = time.time()\n",
    "    batch_end = min(batch_start + batch_size, len(prompts))\n",
    "    current_batch = prompts[batch_start:batch_end]\n",
    "\n",
    "    inputs = model.tokenizer(current_batch, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_output = model.generate(**inputs, **sampling_kwargs)\n",
    "\n",
    "    generated_responses.extend(model.tokenizer.batch_decode(generated_output[:, inputs['input_ids'].size(1):], skip_special_tokens=True))\n",
    "\n",
    "with open(\"gen_or_hard_ftwoc_narafa.jsonl\", 'a', encoding='utf-8') as output_file:\n",
    "    for b in range(len(generated_responses)):\n",
    "        json.dump({\"category\": categories[b], \"question\": prompts[b], \"response\": generated_responses[b]}, output_file)\n",
    "        output_file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_path = base_model_path#\"cackerman/ft_randalias_0to31_interleaved_stdmixsafecombo6_none_mult0\"#ft_0to31_interleaved_stdmixsafecombonarafa_none_mult0\"#\n",
    "#model = load_model(model_path, base_model_path, bnb = False)\n",
    "model.tokenizer.padding_side = \"left\"\n",
    "model.eval()\n",
    "sampling_kwargs = {\"use_cache\": True, \"pad_token_id\": model.tokenizer.eos_token_id, \"max_new_tokens\": 120, \"do_sample\": False, \"top_p\": None, \"temperature\": None}\n",
    "\"\"\"\n",
    "harmful1_file_path = './refusal_cln.json'\n",
    "harmful_responses, prompts = [], []\n",
    "with open(harmful1_file_path, 'r') as file:\n",
    "    data = json.load(file) \n",
    "    for item in data:\n",
    "        prompts.append(item['question'])\n",
    "        harmful_responses.append({'question': item['question'], 'answer': item['answer_harmful']})\n",
    "\"\"\"\n",
    "\"\"\"        \n",
    "harmful2_file_path = './harmful_lat.json'\n",
    "harmful_lat_responses, prompts = [], []\n",
    "with open(harmful2_file_path, 'r') as file:\n",
    "    data = json.load(file) \n",
    "    for item in data:\n",
    "        prompts.append(item['question'])\n",
    "        harmful_lat_responses.append({'question': item['question'], 'answer': item['answer_harmful']})\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "insults_file_path = './insults.jsonl'\n",
    "mean_responses, prompts = [], []\n",
    "with open(insults_file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        d = json.loads(line)\n",
    "        prompts.append(d['question'])\n",
    "        mean_responses.append({'question': d['question'], 'answer': d['mean_answer']})\n",
    "\"\"\"\n",
    "harmful3_file_path = './trustai_msj.json'\n",
    "harmful3_responses, prompts = [], []\n",
    "with open(harmful3_file_path, 'r') as file:\n",
    "    data = json.load(file) \n",
    "    for item in data:\n",
    "        if item['category'].startswith(\"Harmful\"): \n",
    "            prompts.append(item['user'])\n",
    "            harmful3_responses.append({'category': item['category'], 'question': item['user'], 'answer': item['assistant']})\n",
    "            \n",
    "prompts = [f\"<|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" for prompt in prompts]\n",
    "batch_size=32\n",
    "generated_responses=[]\n",
    "for batch_start in range(0, len(prompts), batch_size):\n",
    "    print(f\"Starting {batch_start}\")\n",
    "    batch_start_time = time.time()\n",
    "    batch_end = min(batch_start + batch_size, len(prompts))\n",
    "    current_batch = prompts[batch_start:batch_end]\n",
    "\n",
    "    inputs = model.tokenizer(current_batch, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated_output = model.generate(**inputs, **sampling_kwargs)\n",
    "\n",
    "    generated_responses.extend(model.tokenizer.batch_decode(generated_output[:, inputs['input_ids'].size(1):], skip_special_tokens=True))\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"user\": harmful3_responses[b]['question'],\n",
    "        \"assistant\": harmful3_responses[b]['answer'],\n",
    "        \"assistant_harmless\": generated_responses[b]\n",
    "        ,\"category\": harmful3_responses[b]['category']\n",
    "\n",
    "    }\n",
    "    for b in range(len(generated_responses))\n",
    "]\n",
    "\n",
    "with open(\"trustai_msj_l38b.json\", 'w', encoding='utf-8') as output_file:\n",
    "    json.dump(data, output_file, indent=2)\n",
    "\"\"\"\n",
    "\n",
    "with open(\"insults_l38b.jsonl\", 'a', encoding='utf-8') as output_file:\n",
    "    for b in range(len(generated_responses)):\n",
    "        json.dump({\n",
    "            \"question\": prompts[b],\n",
    "            \"mean_answer\": mean_responses[b]['answer'],\n",
    "            \"normal_answer\": generated_responses[b]\n",
    "        }, output_file)\n",
    "        output_file.write('\\n')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
