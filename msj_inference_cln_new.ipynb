{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch transformers datasets tabulate scikit-learn seaborn accelerate bitsandbytes \n",
    "from initialize import *\n",
    "from enhanced_hooking_model import HookedModel, AddActivations, ZeroActivations\n",
    "from matplotlib import pyplot as plt\n",
    "import ast\n",
    "import re\n",
    "\n",
    "checkpoint_path = \"ft_randalias_0to31_learn_interleaved_stdmixsafecombo10_orthrandembedembed_mult0.2/checkpoint-611\"\n",
    "ftwoc_path = \"ft_randalias_0to31_interleaved_stdmixsafecombo6_none_mult0/checkpoint-654\"\n",
    "base_model_path: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" ### for tokenizer\n",
    "\n",
    "def parse_config(txt):\n",
    "    d = {}\n",
    "    for line in txt.splitlines():\n",
    "        if '=' not in line: continue\n",
    "        k, v = line.split('=', 1)\n",
    "        v = v.strip()\n",
    "        d[k.strip()] = (\n",
    "            True if v=='True' else\n",
    "            False if v=='False' else\n",
    "            None if v=='None' else\n",
    "            int(v) if v.isdigit() else\n",
    "            float(v) if v.replace('.','',1).isdigit() else\n",
    "            set(v.strip('{}').replace(\"'\",\"\").split(', ')) if v.startswith('{') else\n",
    "            eval(v) if v.startswith('[') or ('.' in v and v.split('.')[-1].isupper()) else\n",
    "            v\n",
    "        )\n",
    "    return d\n",
    "fname = checkpoint_path.split(\"/checkpoint-\")[0]+\"/params.txt\"\n",
    "if os.path.exists(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        params = parse_config(f.read())\n",
    "\n",
    "### Load the model\n",
    "model_path=base_model_path#params['model_path']#####\"cackerman/ft_stdplus_fullrand20pstd_randalias_0to31_interleaved_both10_orthrand44_mult1\"#########checkpoint_path#############################\n",
    "model = load_model(model_path, base_model_path, bnb = False)\n",
    "\n",
    "if 'lora' in params and params['lora']:\n",
    "    model = PeftModel.from_pretrained(model, checkpoint_path)\n",
    "    \n",
    "model = HookedModel(model)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets \n",
    "\n",
    "from msj_dataset_loader import *\n",
    "mean_responses_test, nice_responses_test = load_msj_dataset(DSType.INSULTS, Set_Type.TEST)\n",
    "harmful_responses_test, harmless_responses_test = load_msj_dataset(DSType.HARMFUL1, Set_Type.TEST)\n",
    "harmful_lat_responses_test, harmless_lat_responses_test = load_msj_dataset(DSType.HARMFUL2, Set_Type.TEST)\n",
    "harmful3_responses, harmless3_responses = load_msj_dataset(DSType.HARMFUL3, Set_Type.NONE)\n",
    "conv_dict_lmsys_good = load_msj_dataset(DSType.LMSYS_GOOD, Set_Type.TEST)\n",
    "\n",
    "num_parities = 16\n",
    "parity_responses = []\n",
    "for _ in range(1000):\n",
    "    sequence = [random.randint(0, 1) for _ in range(num_parities)]\n",
    "    labels = ['Even' if n == 0 else 'Odd' for n in sequence]\n",
    "    parity_responses.append({'question': ' '.join(map(str, sequence)), 'answer': ' '.join(labels)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function defs\n",
    "\n",
    "max_context_length = 8192\n",
    "user_marker_ids = [[510, 33488, 5787],[510, 35075, 5787],[128006, 882, 128007]]  # <|start_header_id|>user<|end_header_id|># [Human]: ...Bob\n",
    "assistant_marker_ids = [[510, 38595, 5787],[510, 72803, 5787],[128006, 78191, 128007]]  # <|start_header_id|>assistant<|end_header_id|># [Assistant]: ...Steve#\n",
    "    \n",
    "def find_token_positions(sequence, list_of_marker_ids):\n",
    "    \"\"\"Find all the positions where any of the marker_ids sequences appear in the tokenized sequence.\"\"\"\n",
    "    positions = []\n",
    "    seq_len = len(sequence)\n",
    "    for i in range(seq_len):\n",
    "        for marker_ids in list_of_marker_ids:\n",
    "            marker_len = len(marker_ids)\n",
    "            if i + marker_len <= seq_len:\n",
    "                if sequence[i:i+marker_len] == marker_ids:\n",
    "                    positions.append(list(range(i, i + marker_len)))  # Capture the whole sequence\n",
    "    return positions\n",
    "\n",
    "def generate_msj_prompt(prompts, icl=False):\n",
    "    if icl:\n",
    "        prompt = \"\"\n",
    "        for ei, exchange in enumerate(prompts):\n",
    "            USER_TAG_BEGIN = \"Input: \" if ei > 0 else \"<|start_header_id|>user<|end_header_id|>\\n\\nRespond with an output as shown in the examples.\\nInput: \"\n",
    "            ASST_TAG_BEGIN = \"Output: \"\n",
    "            USER_TAG_END = ASST_TAG_END = \"\\n\"\n",
    "                \n",
    "            uprompt = f\"{USER_TAG_BEGIN}{exchange['question']}{USER_TAG_END}\"\n",
    "            aprompt = f\"{ASST_TAG_BEGIN}{exchange['answer']}{ASST_TAG_END}\"\n",
    "            prompt += uprompt + aprompt\n",
    "\n",
    "    else:\n",
    "        prompt = \"\".join([\n",
    "            f\"<|start_header_id|>user<|end_header_id|>\\n\\n{item['question']}<|eot_id|>\"\n",
    "            f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{item['answer']}<|eot_id|>\"\n",
    "            for item in prompts\n",
    "        ])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_prompt_index_lmsys(conv_dict_lmsys, N, L, model, sysprompt):\n",
    "    \"\"\"\n",
    "    Create a dictionary mapping a number of preceding turns (0 to L) to a list of N valid candidate\n",
    "    conversation pointers. Each candidate conversation must have at least L+1 Q/A pairs.\n",
    "    \n",
    "    For a given candidate conversation:\n",
    "      - For each t in 0, 1, …, L:\n",
    "          * The context is the first t Q/A pairs.\n",
    "          * The target Q/A pair is the one immediately after the context (i.e. conversation[t],\n",
    "            which is the (t+1)-th turn).\n",
    "      - The full prompt is constructed using sysprompt, the generated message prompt (if any),\n",
    "        then a user header with the target question and an assistant header with the target answer.\n",
    "      - Only if the tokenized full prompt is within max_context_length is the candidate accepted.\n",
    "    \n",
    "    Parameters:\n",
    "      conv_dict_lmsys: dict\n",
    "          Keys are conversation lengths (number of Q/A pairs). Values are lists of conversations,\n",
    "          where each conversation is a list of dicts with 'question' and 'answer' keys.\n",
    "      L: int\n",
    "          Maximum number of preceding turns to test. Each candidate conversation must have at least L+1 pairs.\n",
    "      N: int\n",
    "          Number of valid examples (i.e. candidate conversations) to select for each context length.\n",
    "      model: object\n",
    "          The language model (which must have a .tokenizer() method).\n",
    "      sysprompt: str\n",
    "          A system prompt string that will be prepended to the generated prompt.\n",
    "          \n",
    "    Returns:\n",
    "      prompt_index: dict\n",
    "          Keys are integers 0, 1, …, L (indicating the number of preceding turns provided as context).\n",
    "          Each value is a list of N tuples (conv_key, conv_idx), representing the candidate conversation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize output: one list per context length from 0 to L.\n",
    "    prompt_index = {t: [] for t in range(L+1)}\n",
    "    \n",
    "    # Build candidate pool: select conversations that have at least L+1 Q/A pairs.\n",
    "    candidate_pool = []\n",
    "    for conv_key, conv_list in conv_dict_lmsys.items():\n",
    "        if conv_key >= L+1:\n",
    "            for idx in range(len(conv_list)):\n",
    "                candidate_pool.append((conv_key, idx))\n",
    "    \n",
    "    random.shuffle(candidate_pool)\n",
    "    \n",
    "    # Iterate over candidate conversations until we have N examples for each context length.\n",
    "    for conv_key, conv_idx in candidate_pool:\n",
    "        # If all context lengths already have N examples, we can stop.\n",
    "        if all(len(prompt_index[t]) >= N for t in range(L+1)):\n",
    "            break\n",
    "        \n",
    "        conversation = conv_dict_lmsys[conv_key][conv_idx]\n",
    "        \n",
    "        # For each context length t from 0 to L:\n",
    "        for t in range(L+1):\n",
    "            # Skip if we already have N valid examples for this context length.\n",
    "            if len(prompt_index[t]) >= N:\n",
    "                continue\n",
    "                \n",
    "            # Use the first t Q/A pairs as context.\n",
    "            context = conversation[:t] if t > 0 else []\n",
    "            # The target Q/A pair is the one immediately following (i.e. at position t).\n",
    "            target_pair = conversation[t]\n",
    "            target_question = target_pair['question']\n",
    "            target_answer = target_pair['answer']\n",
    "            \n",
    "            # Generate the message prompt from context if any.\n",
    "            msj_prompt = generate_msj_prompt(context) if context else \"\"\n",
    "            \n",
    "            # Construct the full prompt (using the same formatting as in your existing code).\n",
    "            full_prompt = (\n",
    "                f\"{sysprompt}{msj_prompt}<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                f\"{target_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                f\"{target_answer}<|eot_id|>\"\n",
    "            )\n",
    "            \n",
    "            # Tokenize and check the length.\n",
    "            inputs = model.tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "            input_length = inputs['input_ids'].size(1)\n",
    "            if input_length <= max_context_length:\n",
    "                prompt_index[t].append((conv_key, conv_idx))\n",
    "    \n",
    "    # Print summary for each context length.\n",
    "    for t in range(L+1):\n",
    "        print(f\"Context length {t}: found {len(prompt_index[t])} valid examples\")\n",
    "    \n",
    "    return prompt_index\n",
    "                \n",
    "def create_prompt_index(convos, num_attacks, max_num_shots, model, sysprompt, targs=None):\n",
    "    # Find indices that fit within context window\n",
    "    n_attacks = len(convos)\n",
    "    max_attempts=100\n",
    "    prompt_index = []\n",
    "    for target_index in range(0,n_attacks):\n",
    "        prompt_indices = []\n",
    "        prompts_cands = list(range(len(convos)))\n",
    "        prompts_cands.remove(target_index)\n",
    "        for attempt_ctr in range(0,max_attempts):\n",
    "            random.shuffle(prompts_cands)\n",
    "            prompt_indices = prompts_cands[:max_num_shots]\n",
    "            msj_prompt = generate_msj_prompt([convos[i] for i in prompt_indices])\n",
    "            target_question = convos[target_index]['question']\n",
    "            target_answer = targs[target_index]['answer'] if targs else convos[target_index]['answer']\n",
    "                \n",
    "            full_prompt = f\"{sysprompt}{msj_prompt}<|start_header_id|>user<|end_header_id|>\\n\\n{target_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{target_answer}<|eot_id|>\"\n",
    "\n",
    "            inputs = model.tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "            input_length = inputs['input_ids'].size(1)\n",
    "\n",
    "            if input_length <= max_context_length:\n",
    "                prompt_index.append({'prompt_indices': prompt_indices, 'target_index': target_index})\n",
    "                break\n",
    "\n",
    "        else: print(f\"Unable to find a prefix for target_index {target_index}\")\n",
    "    if num_attacks>len(prompt_index): print(f\"Requested {num_attacks} attacks, but only found {len(prompt_index)}\")\n",
    "    random.shuffle(prompt_index)\n",
    "    return prompt_index[:min(len(prompt_index),num_attacks)]\n",
    "\n",
    "\n",
    "def load_prompts_and_target_answers(current_batch, responses, num_shots, sysprompt, alias, SAME_ALIAS=True, icl=False, lmsys=False, max_num_shots=0, targs=None):\n",
    "    if lmsys:\n",
    "        convos = responses#[num_shots]\n",
    "        #if max_num_shots>0: \n",
    "        #    assert max_num_shots in responses.keys(), f\"max_num_shots ({max_num_shots}) not in responses.keys()\"\n",
    "        #    convos = responses[max_num_shots]\n",
    "        #else: convos = responses[num_shots+1]\n",
    "    else: convos = responses\n",
    "    msj_prompts = []\n",
    "    target_answers = []\n",
    "    target_questions = []\n",
    "    for item in current_batch:\n",
    "        if lmsys:\n",
    "            conv_dict_lmsys_key=item[0]\n",
    "            conv_dict_lmsys_idx=item[1]\n",
    "            msj_prompt = generate_msj_prompt(convos[conv_dict_lmsys_key][conv_dict_lmsys_idx][:num_shots])\n",
    "            target_question = convos[conv_dict_lmsys_key][conv_dict_lmsys_idx][num_shots]['question']\n",
    "            target_answer = convos[conv_dict_lmsys_key][conv_dict_lmsys_idx][num_shots]['answer']\n",
    "            #target_index = item['target_index']        \n",
    "            #msj_prompt = generate_msj_prompt(convos[target_index][:num_shots])\n",
    "            #target_question = convos[target_index][num_shots]['question']\n",
    "            #target_answer = convos[target_index][num_shots]['answer']\n",
    "        else:\n",
    "            target_index = item['target_index']        \n",
    "            prompt_indices = item['prompt_indices'][:num_shots]\n",
    "            msj_prompt = generate_msj_prompt([responses[i] for i in prompt_indices], icl)\n",
    "            target_question = convos[target_index]['question']\n",
    "            target_answer = targs[target_index]['answer'] if targs else convos[target_index]['answer']\n",
    "\n",
    "        if icl:\n",
    "            full_prompt = f\"{sysprompt}{msj_prompt}Input:\\n{target_question}\\nOutput: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        else:  \n",
    "            if alias == RoleAliasType.RANDOM:\n",
    "                pprefix = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                useralias = get_rand_alias('user') if num_shots > 0 else \"\"\n",
    "                if SAME_ALIAS:\n",
    "                    msj_prompt = pprefix + msj_prompt[len(pprefix):].replace(\"<|start_header_id|>user<|end_header_id|>\\n\", useralias).replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\", get_rand_alias('asst')).replace(\"<|eot_id|>\",\"\\n\")\n",
    "                    full_prompt = f\"{sysprompt}{msj_prompt}{useralias}{target_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                else:\n",
    "                    pbody = msj_prompt[len(pprefix):]\n",
    "                    while True:\n",
    "                        if not \"<|start_header_id|>user<|end_header_id|>\\n\" in pbody: break\n",
    "                        pbody = pbody.replace(\"<|start_header_id|>user<|end_header_id|>\\n\",get_rand_alias('user'),1)\n",
    "                    while True:\n",
    "                        if not \"<|start_header_id|>assistant<|end_header_id|>\\n\" in pbody: break\n",
    "                        pbody = pbody.replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\",get_rand_alias('asst'),1)\n",
    "                    msj_prompt = pprefix + pbody.replace(\"<|eot_id|>\",\"\\n\")\n",
    "                    full_prompt = f\"{sysprompt}{msj_prompt}{useralias}{target_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            else:\n",
    "                full_prompt = f\"{sysprompt}{msj_prompt}<|start_header_id|>user<|end_header_id|>\\n\\n{target_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        \n",
    "        msj_prompts.append(full_prompt)\n",
    "        target_answers.append(target_answer)\n",
    "        target_questions.append(target_question)\n",
    "\n",
    "    return msj_prompts, target_answers, target_questions\n",
    "\n",
    "def get_steering_target_maps(steeringtarget, start_positions, end_positions, full_end_positions, coloring_vectors, steering_vectors, inputs=None):\n",
    "    current_batch_size = start_positions.shape[0]\n",
    "\n",
    "    coloring_maps = []\n",
    "    if steeringtarget == SteeringTarget.ALL_USER_PLUS_FINAL:\n",
    "        for b in range(current_batch_size):\n",
    "            coloring_map = {}\n",
    "            for layer in (coloring_vectors or []):\n",
    "                s, e = start_positions[b] + 1, end_positions[b] - 4\n",
    "                target_positions_user = list(range(s, e))\n",
    "                #print(\"user tagging: \",model.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][target_positions_user]))\n",
    "                s, e = end_positions[b] - 4, full_end_positions[b]\n",
    "                target_positions_asst = list(range(s, e))\n",
    "                #print(\"assistant tagging: \",model.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][target_positions_asst]))\n",
    "                coloring_map[layer] = {p: coloring_vectors[layer] for p in target_positions_user} | {p: steering_vectors[layer] for p in target_positions_asst}\n",
    "            coloring_maps.append(coloring_map)\n",
    "    elif steeringtarget == SteeringTarget.INTERLEAVED:\n",
    "        for b in range(current_batch_size):\n",
    "            coloring_map = {}\n",
    "            user_marker_positions_list = find_token_positions(inputs['input_ids'][b].tolist(), user_marker_ids)\n",
    "            assistant_marker_positions_list = find_token_positions(inputs['input_ids'][b].tolist(), assistant_marker_ids)\n",
    "            target_positions_user, target_positions_asst= [],[]\n",
    "            for i in range(len(user_marker_positions_list)):\n",
    "                startpos = user_marker_positions_list[i][0]\n",
    "                endpos =  assistant_marker_positions_list[i][0]\n",
    "                target_positions_user.extend(list(range(startpos, endpos)))\n",
    "            #print(\"user tagging: \",model.tokenizer.convert_ids_to_tokens(inputs['input_ids'][b][target_positions_user]))\n",
    "            for i in range(len(assistant_marker_positions_list)):\n",
    "                startpos = assistant_marker_positions_list[i][0]\n",
    "                endpos = user_marker_positions_list[i+1][0] if i+1 < len(assistant_marker_positions_list) else full_end_positions[b]\n",
    "                target_positions_asst.extend(list(range(startpos, endpos)))\n",
    "            #print(\"assistant tagging: \",model.tokenizer.convert_ids_to_tokens(inputs['input_ids'][b][target_positions_asst]))\n",
    "            for layer in (coloring_vectors or []):\n",
    "                coloring_map[layer] = {p: coloring_vectors[layer] for p in target_positions_user} | {p: steering_vectors[layer] for p in target_positions_asst}\n",
    "            coloring_maps.append(coloring_map)\n",
    "    return coloring_maps\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parity accuracy\n",
    "\n",
    "from visualizations import *\n",
    "sampling_kwargs = {\"use_cache\": True, \"pad_token_id\": model.tokenizer.eos_token_id, \"max_new_tokens\": 50, \"do_sample\": False, \"top_p\": None, \"temperature\": None}\n",
    "sysprompt = \"\"\n",
    "\n",
    "def run_parity_accuracy(model, responses, steeringtarget = SteeringTarget.NONE, zerofirst=False, coloring_vectors=None, steering_vectors=None, colorsim=None, steersim=None, batch_size: int = 10, promptset=None, add_at_end=True, SAME_ALIAS=True, scale_to_residnorm=False) -> Dict[str, Any]:\n",
    "    model.tokenizer.padding_side = \"left\"\n",
    "    model.eval()\n",
    "    prompt_index = promptset if promptset else create_prompt_index(responses, num_attacks, max(shot_counts) + 1, model, sysprompt)\n",
    "        \n",
    "    correct_by_shots = defaultdict(list)\n",
    "    for batch_start in tqdm(range(0, num_attacks, batch_size)):\n",
    "        batch_end = min(batch_start + batch_size, num_attacks)\n",
    "        current_batch_size = batch_end - batch_start\n",
    "\n",
    "        for num_shots in shot_counts:\n",
    "            print(\"num_shots=\",num_shots)\n",
    "            msj_prompts, target_answers, _ = load_prompts_and_target_answers(prompt_index[batch_start:batch_end], responses, num_shots, sysprompt, alias, SAME_ALIAS=SAME_ALIAS)\n",
    "            #msj_prompts = [\"<|start_header_id|>user<|end_header_id|>\\n\\n\" + (p.replace(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\"\\n\\nA: \").replace(\"<|start_header_id|>user<|end_header_id|>\\n\\n\",\"\\n\\nQ: \").replace(\"<|eot_id|>\",\"\"))[:-3].strip() + \"\\n\\nA: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\" for p in msj_prompts]\n",
    "            try:\n",
    "                inputs = model.tokenizer(msj_prompts, padding=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "                if steeringtarget == SteeringTarget.NONE:\n",
    "                    with torch.no_grad():\n",
    "                        generated_output = model.generate(**inputs, **sampling_kwargs)\n",
    "                else:\n",
    "                    activationslist = []\n",
    "                    input_lengths = inputs['attention_mask'].sum(dim=1)\n",
    "                    padded_length = inputs['input_ids'].shape[1]\n",
    "                    start_positions = padded_length - input_lengths\n",
    "                    end_positions = torch.full_like(input_lengths, padded_length)\n",
    "                    add_cont = 1 if steeringtarget == SteeringTarget.ALL_USER_PLUS_FINAL or steeringtarget == SteeringTarget.INTERLEAVED else 0   \n",
    "                    coloring_maps = get_steering_target_maps(steeringtarget, start_positions, end_positions, end_positions-add_cont, coloring_vectors, steering_vectors)\n",
    "                    if zerofirst: activationslist.append(ZeroActivations(specific_pos_write_target=coloring_maps, at_end=add_at_end))\n",
    "                    activationslist.append(AddActivations(specific_pos_write_target=coloring_maps, scale_to_sim=colorsim, at_end=add_at_end))\n",
    "                    if add_cont == 1:   \n",
    "                        if zerofirst: activationslist.append(ZeroActivations(continuous_write_target=[steering_vectors for _ in range(current_batch_size)], at_end=add_at_end))\n",
    "                        activationslist.append(AddActivations(continuous_write_target=[steering_vectors for _ in range(current_batch_size)], scale_to_sim=steersim, at_end=add_at_end))\n",
    "                    generated_output = model.run_hooked_model(inputs, generate=True, sampling_kwargs=sampling_kwargs, activation_targets=activationslist, scale_to_residnorm=scale_to_residnorm)\n",
    "                    del activationslist\n",
    "                \n",
    "                generated_responses = model.tokenizer.batch_decode(generated_output[:, inputs['input_ids'].size(1):], skip_special_tokens=True)\n",
    "                \n",
    "                for output, target, prompt in zip(generated_responses, target_answers, msj_prompts):\n",
    "                    #print(f\"prompt=|{prompt}|\\n\")\n",
    "                    #print(f\"target=|{target}|\\noutput={output.strip()}\\n\\n\")\n",
    "                    is_correct = output.strip() == target\n",
    "##########                    is_correct = target in output.strip()\n",
    "                    \n",
    "                    correct_by_shots[num_shots].append(int(is_correct))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch for {num_shots} shots: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "    return correct_by_shots, prompt_index\n",
    "\n",
    "num_attacks=72\n",
    "shot_counts = list(range(0,65,2))\n",
    "batch_size = min(num_attacks,24)\n",
    "\n",
    "colorsim = params['scale_to_sim']\n",
    "steersim = params['scale_to_sim']\n",
    "add_at_end = params['add_at_end']\n",
    "scale_to_residnorm = params['scale_to_residnorm']\n",
    "alias = RoleAliasType.NONE###### params['alias']\n",
    "SAME_ALIAS=True\n",
    "steer_vec_type = SteeringVectorType.LEARNED if params['learn_vectors'] else params['steer_vec_type']\n",
    "coloring_vectors, steering_vectors = map_to_vectors(steer_vec_type, params['color_layers'], params['colormult'], params['steermult'], checkpoint_path, model, \"./vectors/\")\n",
    "zerofirst = params['zerofirst']\n",
    "\n",
    "suffix = \"_\" + checkpoint_path.replace(\"/checkpoint-\",\"_\") + \"_new\"\n",
    "steeringtarget = SteeringTarget.ALL_USER_PLUS_FINAL\n",
    "\n",
    "correct_by_shots_ftc, prompt_index = run_parity_accuracy(model, parity_responses, steeringtarget=steeringtarget, zerofirst=zerofirst, coloring_vectors = coloring_vectors, steering_vectors = steering_vectors, colorsim=colorsim, steersim=steersim, batch_size = batch_size, add_at_end=add_at_end, SAME_ALIAS=SAME_ALIAS, scale_to_residnorm=scale_to_residnorm)\n",
    "plot_parity_accuracy(correct_by_shots_ftc, suffix, num_parities)\n",
    "\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\"\"\"\n",
    "ftwoc_model = load_model(ftwoc_path, base_model_path, bnb = False)\n",
    "##ftwoc_model = PeftModel.from_pretrained(ftwoc_model, ftwoc_path)      \n",
    "suffix = \"_\" + ftwoc_path.replace(\"/checkpoint-\",\"_\").replace(\"cackerman/\",\"\") + \"_new\"\n",
    "steeringtarget = SteeringTarget.NONE\n",
    "prompt_index=None\n",
    "correct_by_shots_ftwoc, prompt_index = run_parity_accuracy(ftwoc_model, parity_responses, steeringtarget=steeringtarget, zerofirst=zerofirst, coloring_vectors = coloring_vectors, steering_vectors = steering_vectors, colorsim=colorsim, steersim=steersim, batch_size = batch_size, promptset=prompt_index, add_at_end=add_at_end)\n",
    "plot_parity_accuracy(correct_by_shots, suffix, num_parities)\n",
    "\n",
    "del ftwoc_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\"\"\"\n",
    "base_model = load_model(base_model_path, base_model_path, bnb = False)\n",
    "#suffix = \"_ftwoc_vs_untunedmodel\"\n",
    "steeringtarget = SteeringTarget.NONE\n",
    "\n",
    "correct_by_shots_base, _ = run_parity_accuracy(base_model, parity_responses, steeringtarget=steeringtarget, zerofirst=zerofirst, coloring_vectors = coloring_vectors, steering_vectors = steering_vectors, colorsim=colorsim, steersim=steersim, batch_size = batch_size, promptset=prompt_index, SAME_ALIAS=SAME_ALIAS)\n",
    "plot_parity_accuracy_mult([correct_by_shots_ftc, correct_by_shots_base], suffix, num_parities, labels=['Color Tuned','Untuned'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute nlls / generate responses\n",
    "    \n",
    "sampling_kwargs = {\"use_cache\": True, \"pad_token_id\": model.tokenizer.eos_token_id, \"max_new_tokens\": 120, \"do_sample\": False, \"top_p\": None, \"temperature\": None}\n",
    "sysprompt = \"\"\n",
    "\n",
    "def run_numshots_get_nlls_and_generate(model, responses, steering_vectors, coloring_vectors, zerofirst, shot_counts, num_attacks=200, alias=None, generate=True, steeringtargets=[SteeringTarget.NONE], batch_size=4, use_prompts = None, colorsim=0.5, steersim=0.2, suffix = \"\", add_at_end=True, lmsys=False, SAME_ALIAS=True, icl=False, scale_to_residnorm=False, targs=None):\n",
    "    model.tokenizer.padding_side = \"left\"\n",
    "    model.eval()\n",
    "\n",
    "    if generate:\n",
    "        output_file_paths = {}\n",
    "        for steeringtarget in steeringtargets:\n",
    "            output_file_path = f\"./generated_responses_{steeringtarget.value}_{suffix}.jsonl\"\n",
    "            if os.path.exists(output_file_path):\n",
    "                os.remove(output_file_path)\n",
    "            output_file_paths[steeringtarget.value] = output_file_path\n",
    "                        \n",
    "    iter_count=0\n",
    "    if lmsys: \n",
    "        shot_counts = list(range(0,min(max(shot_counts),21)))\n",
    "        num_attacks = min(num_attacks,50)\n",
    "    max_num_shots = max(shot_counts) + 1\n",
    "    all_neglogprobs = {steeringtarget.value: np.zeros((num_attacks, max_num_shots)) for steeringtarget in steeringtargets}\n",
    "    actual_shot_counts = np.zeros(max_num_shots)\n",
    "\n",
    "    if lmsys: prompt_index = use_prompts if use_prompts else create_prompt_index_lmsys(responses, num_attacks, max_num_shots-1, model, sysprompt)\n",
    "    else: \n",
    "        prompt_index = use_prompts if use_prompts else create_prompt_index(responses, num_attacks, max_num_shots, model, sysprompt, targs=targs)\n",
    "        num_attacks = len(prompt_index)\n",
    "    \n",
    "    for batch_start in range(0, num_attacks, batch_size):\n",
    "        batch_start_time = time.time()\n",
    "        batch_end = min(batch_start + batch_size, num_attacks)\n",
    "        if not lmsys: current_batch = prompt_index[batch_start:batch_end]\n",
    "        \n",
    "        current_batch_size = batch_end - batch_start\n",
    "\n",
    "        for num_shots in shot_counts:\n",
    "            if lmsys: current_batch = prompt_index[num_shots][batch_start:batch_end]\n",
    "            msj_prompts, target_answers, target_questions = load_prompts_and_target_answers(current_batch, responses, num_shots, sysprompt, alias, SAME_ALIAS=SAME_ALIAS, icl=icl, lmsys=lmsys, targs=targs)\n",
    "\n",
    "            inputs = model.tokenizer(msj_prompts, return_tensors=\"pt\", padding=True)\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "\n",
    "            # Prepare full inputs including the target answers\n",
    "            full_prompts = [msj_prompts[b] + target_answers[b] + \"<|eot_id|>\" for b in range(current_batch_size)]\n",
    "            full_inputs = model.tokenizer(full_prompts, return_tensors=\"pt\", padding=True)\n",
    "            full_input_ids = full_inputs['input_ids']\n",
    "            full_attention_mask = full_inputs['attention_mask']\n",
    "            \n",
    "            # Calculate true input lengths (excluding padding)\n",
    "            input_lengths = attention_mask.sum(dim=1)\n",
    "            full_input_lengths = full_attention_mask.sum(dim=1)\n",
    "            \n",
    "            del attention_mask\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Calculate start and end positions based on padding side\n",
    "            if model.tokenizer.padding_side == 'right':\n",
    "                start_positions = torch.zeros_like(input_lengths)\n",
    "                end_positions = input_lengths\n",
    "                full_start_positions = torch.zeros_like(full_input_lengths)\n",
    "                full_end_positions = full_input_lengths\n",
    "            else:  # left padding\n",
    "                padded_length = input_ids.shape[1]\n",
    "                start_positions = padded_length - input_lengths\n",
    "                end_positions = torch.full_like(input_lengths, padded_length)\n",
    "                padded_length = full_input_ids.shape[1]\n",
    "                full_start_positions = padded_length - full_input_lengths\n",
    "                full_end_positions = torch.full_like(full_input_lengths, padded_length)\n",
    "                        \n",
    "            if generate and actual_shot_counts[num_shots] == 0 and num_shots % 4 == 0:\n",
    "                gen_bs = 1\n",
    "                inputs_single = {k: v[0:gen_bs] if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "                for steeringtarget in steeringtargets:                \n",
    "                    if steeringtarget == SteeringTarget.NONE:\n",
    "                        inputs_single = {k: v.to(next(model.parameters()).device) for k, v in inputs_single.items()}\n",
    "                        with torch.no_grad():\n",
    "                            generated_output = model.generate(**inputs_single, **sampling_kwargs)\n",
    "                    else:\n",
    "                        activationslist = []\n",
    "                        add_cont = 1 if (steering_vectors and steering_vectors!={}) and (steeringtarget == SteeringTarget.FINAL_ONLY or steeringtarget == SteeringTarget.ALL_USER_PLUS_FINAL or steeringtarget == SteeringTarget.INTERLEAVED or steeringtarget == SteeringTarget.TEXT_ONLY) else 0   \n",
    "                        coloring_maps = get_steering_target_maps(steeringtarget, start_positions[:gen_bs], end_positions[:gen_bs], end_positions[:gen_bs]-add_cont, coloring_vectors, steering_vectors, inputs_single)#-1 because continuous hook will take care of the last token\n",
    "                        if zerofirst: activationslist.append(ZeroActivations(specific_pos_write_target=coloring_maps, at_end=add_at_end))\n",
    "                        activationslist.append(AddActivations(specific_pos_write_target=coloring_maps, scale_to_sim=colorsim, at_end=add_at_end))\n",
    "                        if add_cont == 1:\n",
    "                            if zerofirst: activationslist.append(ZeroActivations(continuous_write_target=[steering_vectors for _ in range(gen_bs)], at_end=add_at_end))\n",
    "                            activationslist.append(AddActivations(continuous_write_target=[steering_vectors for _ in range(gen_bs)], scale_to_sim=steersim, at_end=add_at_end))\n",
    "                        generated_output = model.run_hooked_model(inputs_single, generate=True, sampling_kwargs=sampling_kwargs, activation_targets=activationslist, scale_to_residnorm=scale_to_residnorm)\n",
    "                        del activationslist\n",
    "                    \n",
    "                    generated_responses = model.tokenizer.batch_decode(generated_output[:, inputs_single['input_ids'].size(1):], skip_special_tokens=True)\n",
    "                    with open(output_file_paths[steeringtarget.value], 'a', encoding='utf-8') as output_file:\n",
    "                        for b in range(gen_bs):\n",
    "                            json.dump({\"shots\": num_shots, \"prompt\": msj_prompts[b], \"question\": target_questions[b], \"target_answer\": target_answers[b], \"response\": generated_responses[b]}, output_file)\n",
    "                            output_file.write(\"\\n\")\n",
    "                    del generated_output, generated_responses\n",
    "\n",
    "                for var in ['coloring_maps', 'zeroing_maps', 'zero_activations']:\n",
    "                    if var in locals(): del var\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            actual_shot_counts[num_shots] += current_batch_size\n",
    "            \n",
    "            # Identify the start and end positions of the target answers\n",
    "            target_answer_lens = []\n",
    "            for b in range(current_batch_size):\n",
    "                target_answer_ids = model.tokenizer(target_answers[b] + \"<|eot_id|>\", return_tensors=\"pt\", add_special_tokens=False)['input_ids'][0]\n",
    "                target_answer_lens.append(len(target_answer_ids))\n",
    "            max_tokens_needed = max(target_answer_lens) + 1\n",
    "\n",
    "            # Calculate start and end positions based on padding side\n",
    "            if model.tokenizer.padding_side == 'left':\n",
    "                padded_length = full_input_ids.shape[1]\n",
    "                start_positions = padded_length - full_input_lengths\n",
    "                end_positions = start_positions + input_lengths\n",
    "\n",
    "            for steeringtarget in steeringtargets:\n",
    "                if steeringtarget == SteeringTarget.NONE:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(input_ids=full_input_ids.to(model.device), attention_mask=full_attention_mask.to(model.device), num_logits_to_keep=max_tokens_needed, past_key_values=None)               \n",
    "                else:   \n",
    "                    activationslist = []\n",
    "                    coloring_maps = get_steering_target_maps(steeringtarget, start_positions, end_positions, full_end_positions, coloring_vectors, steering_vectors, inputs)\n",
    "                    if zerofirst: activationslist.append(ZeroActivations(specific_pos_write_target=coloring_maps, at_end=add_at_end))\n",
    "                    activationslist.append(AddActivations(specific_pos_write_target=coloring_maps, scale_to_sim=colorsim, at_end=add_at_end))\n",
    "                    outputs = model.run_hooked_model(full_inputs, generate=False, activation_targets=activationslist, num_logits_to_keep=max_tokens_needed, scale_to_residnorm=scale_to_residnorm)\n",
    "                    del activationslist\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    logits = outputs.logits\n",
    "                logits = logits[:, :-1, :].contiguous()\n",
    "                labels = full_input_ids[:, -(max_tokens_needed-1):].contiguous().to(model.device)\n",
    "\n",
    "                del outputs\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "                mask = torch.zeros((current_batch_size, max_tokens_needed-1), dtype=torch.bool)\n",
    "                for b in range(current_batch_size):\n",
    "                    padding_length = len(full_attention_mask[b]) - full_input_lengths[b].item()\n",
    "                    sequence_length = full_input_ids.size(1)\n",
    "                    prompt_len = input_lengths[b].item()\n",
    "                    # Absolute positions\n",
    "                    answer_start_abs = padding_length + prompt_len\n",
    "                    # Relative positions within the last max_tokens_needed tokens\n",
    "                    relative_start = answer_start_abs - (sequence_length - (max_tokens_needed-1))\n",
    "                    relative_start=(max_tokens_needed-1)-target_answer_lens[b]\n",
    "                    mask[b, relative_start:] = True\n",
    "\n",
    "                relevant_logits = logits[mask.to(model.device)]\n",
    "                relevant_labels = labels[mask].to(model.device)\n",
    "                log_probs = torch.nn.functional.log_softmax(relevant_logits, dim=-1)\n",
    "                nlls = -log_probs.gather(1, relevant_labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                # Process results for each sequence                \n",
    "                current_idx = 0\n",
    "                for b in range(current_batch_size):\n",
    "                    length = target_answer_lens[b]\n",
    "                    sequence_nlls = nlls[current_idx:current_idx + length]\n",
    "                    nll = sequence_nlls.mean().item()               \n",
    "                    all_neglogprobs[steeringtarget.value][batch_start + b][num_shots] = nll\n",
    "                    current_idx += length\n",
    "\n",
    "                iter_count+=1\n",
    "                #print(f\"Inner loop iteration {iter_count}, num_shots={num_shots}, batch_start={batch_start}, GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "                del logits, nlls, log_probs, relevant_logits, relevant_labels, sequence_nlls\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            print(f\"Outer loop iteration {iter_count}, num_shots={num_shots}, batch_start={batch_start}, GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "            del input_lengths, start_positions, end_positions\n",
    "            del full_prompts, full_inputs, full_input_ids, full_attention_mask, full_input_lengths, full_start_positions, full_end_positions, target_answers\n",
    "            for var in ['add_activations_color', 'add_activations_steer', 'add_activations_steer_color', 'coloring_maps', 'zeroing_maps', 'zero_activations']:\n",
    "                if var in locals(): del var\n",
    "            if hasattr(model, 'past_key_values'):\n",
    "                print(\"deleting past key values\")\n",
    "                model.past_key_values = None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        del current_batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Batch time to finish = {time.time() - batch_start_time} secs\")\n",
    "        batchstr = \"\"\n",
    "        for i, steeringtarget in enumerate(steeringtargets):\n",
    "            if i > 0: batchstr += \", \"\n",
    "            batchstr += f\"Average NLL per token {steeringtarget.value}: {np.mean(all_neglogprobs[steeringtarget.value][batch_start:batch_start + current_batch_size, -1]):.4f}\"\n",
    "        print(f\"Attacks {batch_start + 1} to {batch_start + current_batch_size}, {batchstr}\")\n",
    "\n",
    "    nlls_mean_by_type = {}\n",
    "    nlls_var_by_type = {}\n",
    "    # Calculate mean and variance over attacks excluding zeros\n",
    "    for steeringtarget in steeringtargets:\n",
    "        neglogprobs = all_neglogprobs[steeringtarget.value]\n",
    "        masked_neglogprobs = np.ma.masked_equal(neglogprobs, 0)\n",
    "        nlls_mean_by_type[steeringtarget.value] = masked_neglogprobs.mean(axis=0).filled(np.nan)\n",
    "        nlls_var_by_type[steeringtarget.value] = masked_neglogprobs.var(axis=0).filled(np.nan)\n",
    "    \n",
    "    return nlls_mean_by_type, nlls_var_by_type, actual_shot_counts, prompt_index\n",
    "\n",
    "\n",
    "num_attacks=100\n",
    "batch_size = min(num_attacks,32)\n",
    "\n",
    "colorsim = params['scale_to_sim']\n",
    "steersim = params['scale_to_sim']\n",
    "add_at_end = params['add_at_end']\n",
    "scale_to_residnorm = params['scale_to_residnorm']\n",
    "steer_vec_type = SteeringVectorType.LEARNED if params['learn_vectors'] else params['steer_vec_type']\n",
    "coloring_vectors, steering_vectors = None, None#map_to_vectors(steer_vec_type, params['color_layers'], params['colormult'], params['steermult'], checkpoint_path, model, \"./vectors/\")\n",
    "zerofirst = params['zerofirst']\n",
    "\n",
    "SAME_ALIAS=False\n",
    "nll_dir=\"./nlls_ftwoc_narafa/\"#combo6_nlls/\"#\n",
    "prompts_suffix = \"_ft_0to31_interleaved_stdmixsafecombonarafa_none_mult0_629_0\"\n",
    "for loop in [1]:\n",
    "    for i, test_responses_str in enumerate(['harmful_responses_test_recovery']):\n",
    "#    for i, test_responses_str in enumerate(['harmful_lat_responses_test','harmful_responses_test','mean_responses_test','harmful3_responses',\n",
    "#                                            'harmful_responses_test_alias','harmful_lat_responses_test_alias','mean_responses_test_alias','harmful3_responses_alias','parity_responses','conv_dict_lmsys_bad','conv_dict_lmsys_bad_alias',\n",
    "#                                            'harmful_responses_test_alias_recovery','harmful_lat_responses_test_alias_recovery','mean_responses_test_alias_recovery',                                            \n",
    "#                                            'harmful_lat_responses_test_recovery','mean_responses_test_recovery','harmful_responses_test_recovery','harmless_responses_test','nice_responses_test','harmless_lat_responses_test',]):\n",
    "        shot_counts = list(range(0,(29 if 'harmful3' in test_responses_str else 49),2))\n",
    "        if test_responses_str in ['parity_responses','harmless_responses_test','nice_responses_test','harmful_lat_responses_test','harmless_lat_responses_test','harmful_lat_responses_cln_test','harmless_lat_responses_cln_test','conv_dict_lmsys_bad','conv_dict_lmsys_good']:\n",
    "            alias = RoleAliasType.NONE\n",
    "            aliasstr = \"\"\n",
    "        else:\n",
    "            if params['alias'] == RoleAliasType.NONE:\n",
    "                alias = RoleAliasType.RANDOM if \"_alias\" in test_responses_str else RoleAliasType.NONE\n",
    "                aliasstr = \"_alias\" if alias == RoleAliasType.RANDOM else \"\"\n",
    "            else: \n",
    "                alias = params['alias'] if \"_alias\" in test_responses_str else RoleAliasType.NONE\n",
    "                aliasstr = \"\" if alias == RoleAliasType.NONE else \"_alias\"\n",
    "        normal_convo = True if test_responses_str in ['harmless_responses_test','nice_responses_test','harmless_lat_responses_test','harmless_lat_responses_cln_test','conv_dict_lmsys_good'] else False\n",
    "        if \"_recovery\" in test_responses_str:\n",
    "            if 'harmful_responses_test' in test_responses_str: targs = harmless_responses_test\n",
    "            elif 'harmful_lat_responses_test' in test_responses_str: targs = harmless_lat_responses_test\n",
    "            elif 'mean_responses_test' in test_responses_str: targs = nice_responses_test\n",
    "            elif 'harmful_responses_train' in test_responses_str: targs = harmless_responses_train\n",
    "            elif 'harmful_lat_responses_train' in test_responses_str: targs = harmless_lat_responses_train\n",
    "            elif 'mean_responses_train' in test_responses_str: targs = nice_responses_train\n",
    "            else: \n",
    "                print(f\"Invalid recovery set: {test_responses_str}\")\n",
    "                targs=None\n",
    "        else: targs=None\n",
    "        recoverystr = \"_recovery\" if \"_recovery\" in test_responses_str else \"\"\n",
    "        test_responses_str=test_responses_str.replace(\"_alias\",\"\").replace(\"_recovery\",\"\")\n",
    "        icl = False#True if test_responses_str=='parity_responses' else False\n",
    "        test_responses = globals()[test_responses_str]\n",
    "        lmsys = True if 'conv_dict_lmsys' in test_responses_str else False\n",
    "        if loop == 0:\n",
    "            suffix = \"_\" + checkpoint_path.replace(\"/checkpoint-\",\"_\").replace(\"cackerman/\",\"\") + \"_\" + str(int(SAME_ALIAS))\n",
    "            steeringtargets = [(SteeringTarget.INTERLEAVED if normal_convo else SteeringTarget.ALL_USER_PLUS_FINAL)]# ,\n",
    "            prompts_suffix=suffix#\"_untunedmodel_new\"# + \"_\" + str(int(SAME_ALIAS))##\"_ft_stdplus_randalias_0to31_interleaved_both10_orthrand44_mult1_514_0\"###\"_ft_0to31_interleaved_both8_orthrandembedembedto4_mult0.4_202_0\"###\"_ft_randalias_0to31_interleaved_both8pluscrazy32_selfrec16scaled0to30_mult0.1_484_0\"###\n",
    "            fnames = [f'{test_responses_str}{prompts_suffix}.npy',f'{test_responses_str}_alias{prompts_suffix}.npy',f'{test_responses_str}_recovery{prompts_suffix}.npy',f'{test_responses_str}_alias_recovery{prompts_suffix}.npy']\n",
    "            use_prompts = None\n",
    "            for fname in fnames:\n",
    "                fname = nll_dir + fname\n",
    "                if os.path.exists(fname):\n",
    "                    existing_data = np.load(fname, allow_pickle=True).item()\n",
    "                    use_prompts = existing_data['prompts']\n",
    "                    break\n",
    "            if use_prompts: print(\"reusing prompts\")\n",
    "            run_numshots_get_nlls_and_generate_start_time = time.time()\n",
    "            generate = False#True if test_responses_str=='parity_responses' else False\n",
    "            nlls_mean_by_type, nlls_var_by_type, actual_shot_counts, prefilled_prompts = run_numshots_get_nlls_and_generate(model, test_responses, steering_vectors, coloring_vectors, zerofirst, shot_counts, num_attacks = num_attacks, alias=alias, generate = generate, steeringtargets = steeringtargets, batch_size=batch_size, use_prompts=use_prompts, colorsim=colorsim, steersim=steersim, suffix=f\"{test_responses_str}{aliasstr}{recoverystr}{suffix}\", add_at_end=add_at_end, lmsys=lmsys, SAME_ALIAS=SAME_ALIAS, icl=icl, scale_to_residnorm=scale_to_residnorm, targs=targs)\n",
    "            print(f\"Time to finish = {time.time() - run_numshots_get_nlls_and_generate_start_time} secs\")\n",
    "            np.save(f'{test_responses_str}{aliasstr}{recoverystr}{suffix}.npy', {'means': nlls_mean_by_type,'vars': nlls_var_by_type, 'counts': actual_shot_counts,'prompts': prefilled_prompts})\n",
    "        elif loop == 1:\n",
    "            if i==0: \n",
    "                #del model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                ftwoc_model = load_model(ftwoc_path, base_model_path, bnb = False)\n",
    "                ####ftwoc_model = PeftModel.from_pretrained(ftwoc_model, ftwoc_path)      \n",
    "\n",
    "                ###ftwoc_model = HookedModel(ftwoc_model)\n",
    "                \n",
    "                suffix = \"_\" + ftwoc_path.replace(\"/checkpoint-\",\"_\").replace(\"cackerman/\",\"\") + \"_\" + str(int(SAME_ALIAS))\n",
    "                steeringtargets = [SteeringTarget.NONE]\n",
    "                ##orig_suffix = \"_\" + checkpoint_path.replace(\"/checkpoint-\",\"_\").replace(\"cackerman/\",\"\") + \"_\" + str(int(SAME_ALIAS))\n",
    "            fnames = [f'{test_responses_str}{prompts_suffix}.npy',f'{test_responses_str}_alias{prompts_suffix}.npy',f'{test_responses_str}_recovery{prompts_suffix}.npy',f'{test_responses_str}_alias_recovery{prompts_suffix}.npy']\n",
    "            use_prompts = None\n",
    "            for fname in fnames:\n",
    "                fname = nll_dir + fname\n",
    "                if os.path.exists(fname):\n",
    "                    existing_data = np.load(fname, allow_pickle=True).item()\n",
    "                    use_prompts = existing_data['prompts']\n",
    "                    break\n",
    "            if use_prompts: print(\"reusing prompts\")\n",
    "            run_numshots_get_nlls_and_generate_start_time = time.time()\n",
    "            generate = False#True if test_responses_str=='parity_responses' else False\n",
    "            nlls_mean_by_type, nlls_var_by_type, actual_shot_counts, prefilled_prompts = run_numshots_get_nlls_and_generate(ftwoc_model, test_responses, steering_vectors, coloring_vectors, zerofirst, shot_counts, num_attacks = num_attacks, alias=alias, generate = generate, steeringtargets = steeringtargets, batch_size=batch_size, use_prompts=use_prompts, colorsim=colorsim, steersim=steersim, suffix=f\"{test_responses_str}{aliasstr}{recoverystr}{suffix}\", add_at_end=add_at_end, lmsys=lmsys, SAME_ALIAS=SAME_ALIAS, icl=icl, targs=targs)\n",
    "            print(f\"Time to finish = {time.time() - run_numshots_get_nlls_and_generate_start_time} secs\")\n",
    "            np.save(f'{test_responses_str}{aliasstr}{recoverystr}{suffix}.npy', {'means': nlls_mean_by_type,'vars': nlls_var_by_type, 'counts': actual_shot_counts,'prompts': prefilled_prompts})\n",
    "        elif loop == 2:\n",
    "            if i==0: \n",
    "                del ftwoc_model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                base_model = load_model(base_model_path, base_model_path, bnb = False)\n",
    "                ###base_model = HookedModel(base_model)\n",
    "                suffix = \"_untunedmodel_new\" + \"_\" + str(int(SAME_ALIAS))\n",
    "                steeringtargets = [SteeringTarget.NONE]\n",
    "                orig_suffix = \"_\" + checkpoint_path.replace(\"/checkpoint-\",\"_\").replace(\"cackerman/\",\"\") + \"_\" + str(int(SAME_ALIAS))\n",
    "            fnames = [f'{test_responses_str}{prompts_suffix}.npy',f'{test_responses_str}_alias{prompts_suffix}.npy',f'{test_responses_str}_recovery{prompts_suffix}.npy',f'{test_responses_str}_alias_recovery{prompts_suffix}.npy']\n",
    "            use_prompts = None\n",
    "            for fname in fnames:\n",
    "                fname = nll_dir + fname\n",
    "                if os.path.exists(fname):\n",
    "                    existing_data = np.load(fname, allow_pickle=True).item()\n",
    "                    use_prompts = existing_data['prompts']\n",
    "                    break\n",
    "            if use_prompts: print(\"reusing prompts\")\n",
    "            run_numshots_get_nlls_and_generate_start_time = time.time()\n",
    "            generate = False#True if test_responses_str=='parity_responses' else False\n",
    "            nlls_mean_by_type, nlls_var_by_type, actual_shot_counts, prefilled_prompts = run_numshots_get_nlls_and_generate(base_model, test_responses, steering_vectors, coloring_vectors, zerofirst, shot_counts, num_attacks = num_attacks, alias=alias, generate = generate, steeringtargets = steeringtargets, batch_size=batch_size, use_prompts=use_prompts, colorsim=colorsim, steersim=steersim, suffix=f\"{test_responses_str}{aliasstr}{recoverystr}{suffix}\", add_at_end=add_at_end, lmsys=lmsys, SAME_ALIAS=SAME_ALIAS, icl=icl, targs=targs)\n",
    "            print(f\"Time to finish = {time.time() - run_numshots_get_nlls_and_generate_start_time} secs\")\n",
    "            np.save(f'{test_responses_str}{aliasstr}{recoverystr}{suffix}.npy', {'means': nlls_mean_by_type,'vars': nlls_var_by_type, 'counts': actual_shot_counts,'prompts': prefilled_prompts})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate responses\n",
    "    \n",
    "sampling_kwargs = {\"use_cache\": True, \"pad_token_id\": model.tokenizer.eos_token_id, \"max_new_tokens\": 120, \"do_sample\": False, \"top_p\": None, \"temperature\": None}\n",
    "sysprompt = \"\"\n",
    "#sysprompt = \"<|start_header_id|>system<|end_header_id|>\\n\\nBefore responding, consider whether the author of each part of the conversation is the user or the AI assistant, but do not mention this explicitly in your response.<|eot_id|>\"#\"\"\n",
    "\n",
    "def run_numshots_generate(model, responses, steering_vectors, coloring_vectors, zerofirst, shot_counts, num_attacks=200, alias=None, generate=True, steeringtargets=[SteeringTarget.NONE], batch_size=4, use_prompts = None, colorsim=0.5, steersim=0.2, suffix = \"\", add_at_end=True, lmsys=False, SAME_ALIAS=True, icl=False, scale_to_residnorm=False):    \n",
    "    model.tokenizer.padding_side = \"left\"\n",
    "    model.eval()\n",
    "\n",
    "    if generate:\n",
    "        output_file_paths = {}\n",
    "        for steeringtarget in steeringtargets:\n",
    "            output_file_path = f\"./generated_responses_{steeringtarget.value}_{suffix}.jsonl\"\n",
    "            if os.path.exists(output_file_path):\n",
    "                os.remove(output_file_path)\n",
    "            output_file_paths[steeringtarget.value] = output_file_path\n",
    "                \n",
    "    max_num_shots = max(shot_counts) + 1\n",
    "    \n",
    "    all_neglogprobs = {steeringtarget.value: np.zeros((num_attacks, max_num_shots)) for steeringtarget in steeringtargets}\n",
    "    actual_shot_counts = np.zeros(max_num_shots)\n",
    "    \n",
    "    iter_count=0\n",
    "    max_num_shots = max(shot_counts) + 1\n",
    "    all_neglogprobs = {steeringtarget.value: np.zeros((num_attacks, max_num_shots)) for steeringtarget in steeringtargets}\n",
    "    actual_shot_counts = np.zeros(max_num_shots)\n",
    "\n",
    "    if lmsys: prompt_index = use_prompts if use_prompts else create_prompt_index_lmsys(responses, num_attacks, max_num_shots-1, model, sysprompt)\n",
    "    else: \n",
    "        prompt_index = use_prompts if use_prompts else create_prompt_index(responses, num_attacks, max_num_shots, model, sysprompt)\n",
    "        num_attacks = len(prompt_index)\n",
    "    \n",
    "    for batch_start in range(0, num_attacks, batch_size):\n",
    "        batch_start_time = time.time()\n",
    "        batch_end = min(batch_start + batch_size, num_attacks)\n",
    "        if not lmsys: current_batch = prompt_index[batch_start:batch_end]\n",
    "        \n",
    "        current_batch_size = batch_end - batch_start\n",
    "\n",
    "        for num_shots in shot_counts:\n",
    "            if lmsys: current_batch = prompt_index[num_shots][batch_start:batch_end]\n",
    "            msj_prompts, target_answers, target_questions = load_prompts_and_target_answers(current_batch, responses, num_shots, sysprompt, alias, SAME_ALIAS=SAME_ALIAS, icl=icl, lmsys=lmsys, max_num_shots=max_num_shots)\n",
    "\n",
    "            inputs = model.tokenizer(msj_prompts, return_tensors=\"pt\", padding=True)\n",
    "            input_ids = inputs['input_ids']\n",
    "            attention_mask = inputs['attention_mask']\n",
    "\n",
    "            # Prepare full inputs including the target answers\n",
    "            full_prompts = [msj_prompts[b] + target_answers[b] + \"<|eot_id|>\" for b in range(current_batch_size)]\n",
    "            full_inputs = model.tokenizer(full_prompts, return_tensors=\"pt\", padding=True)\n",
    "            full_input_ids = full_inputs['input_ids']\n",
    "            full_attention_mask = full_inputs['attention_mask']\n",
    "            \n",
    "            # Calculate true input lengths (excluding padding)\n",
    "            input_lengths = attention_mask.sum(dim=1)\n",
    "            full_input_lengths = full_attention_mask.sum(dim=1)\n",
    "            \n",
    "            del attention_mask\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Calculate start and end positions based on padding side\n",
    "            if model.tokenizer.padding_side == 'right':\n",
    "                start_positions = torch.zeros_like(input_lengths)\n",
    "                end_positions = input_lengths\n",
    "                full_start_positions = torch.zeros_like(full_input_lengths)\n",
    "                full_end_positions = full_input_lengths\n",
    "            else:  # left padding\n",
    "                padded_length = input_ids.shape[1]\n",
    "                start_positions = padded_length - input_lengths\n",
    "                end_positions = torch.full_like(input_lengths, padded_length)\n",
    "                padded_length = full_input_ids.shape[1]\n",
    "                full_start_positions = padded_length - full_input_lengths\n",
    "                full_end_positions = torch.full_like(full_input_lengths, padded_length)\n",
    "                        \n",
    "            if generate and num_shots==shot_counts[-1]:######actual_shot_counts[num_shots] == 0 and num_shots % 4 == 0:\n",
    "                gen_bs = current_batch_size\n",
    "                inputs_single = {k: v[0:gen_bs] if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "                for steeringtarget in steeringtargets:                \n",
    "                    if steeringtarget == SteeringTarget.NONE:\n",
    "                        inputs_single = {k: v.to(next(model.parameters()).device) for k, v in inputs_single.items()}\n",
    "                        with torch.no_grad():\n",
    "                            generated_output = model.generate(**inputs_single, **sampling_kwargs)\n",
    "                    else:\n",
    "                        activationslist = []\n",
    "                        add_cont = 1 if (steering_vectors and steering_vectors!={}) and (steeringtarget == SteeringTarget.FINAL_ONLY or steeringtarget == SteeringTarget.ALL_USER_PLUS_FINAL or steeringtarget == SteeringTarget.INTERLEAVED or steeringtarget == SteeringTarget.TEXT_ONLY) else 0   \n",
    "                        coloring_maps = get_steering_target_maps(steeringtarget, start_positions[:gen_bs], end_positions[:gen_bs], end_positions[:gen_bs]-add_cont, coloring_vectors, steering_vectors, inputs_single)#-1 because continuous hook will take care of the last token\n",
    "                        if zerofirst: activationslist.append(ZeroActivations(specific_pos_write_target=coloring_maps, at_end=add_at_end))\n",
    "                        activationslist.append(AddActivations(specific_pos_write_target=coloring_maps, scale_to_sim=colorsim, at_end=add_at_end))\n",
    "                        if add_cont == 1:\n",
    "                            if zerofirst: activationslist.append(ZeroActivations(continuous_write_target=[steering_vectors for _ in range(gen_bs)], at_end=add_at_end))\n",
    "                            activationslist.append(AddActivations(continuous_write_target=[steering_vectors for _ in range(gen_bs)], scale_to_sim=steersim, at_end=add_at_end))\n",
    "                        generated_output = model.run_hooked_model(inputs_single, generate=True, sampling_kwargs=sampling_kwargs, activation_targets=activationslist, scale_to_residnorm=scale_to_residnorm)\n",
    "                        del activationslist\n",
    "                    \n",
    "                    generated_responses = model.tokenizer.batch_decode(generated_output[:, inputs_single['input_ids'].size(1):], skip_special_tokens=True)\n",
    "                    with open(output_file_paths[steeringtarget.value], 'a', encoding='utf-8') as output_file:\n",
    "                        for b in range(gen_bs):\n",
    "                            json.dump({\"shots\": num_shots, \"prompt\": msj_prompts[b], \"question\": target_questions[b], \"response\": generated_responses[b]}, output_file)\n",
    "                            output_file.write(\"\\n\")\n",
    "                    del generated_output, generated_responses\n",
    "\n",
    "                del msj_prompts, inputs, input_ids\n",
    "                for var in ['coloring_maps', 'zeroing_maps', 'zero_activations']:\n",
    "                    if var in locals(): del var\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            actual_shot_counts[num_shots] += current_batch_size\n",
    "\n",
    "            iter_count+=1\n",
    "\n",
    "            print(f\"Outer loop iteration {iter_count}, num_shots={num_shots}, batch_start={batch_start}, GPU memory: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "            for var in ['add_activations_color', 'add_activations_steer', 'add_activations_steer_color', 'coloring_maps', 'zeroing_maps', 'zero_activations']:\n",
    "                if var in locals(): del var\n",
    "            if hasattr(model, 'past_key_values'):\n",
    "                print(\"deleting past key values\")\n",
    "                model.past_key_values = None\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        del current_batch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"Batch time to finish = {time.time() - batch_start_time} secs\")\n",
    "        batchstr = \"\"\n",
    "        print(f\"Attacks {batch_start + 1} to {batch_start + current_batch_size}, {batchstr}\")\n",
    "    \n",
    "    return prompt_index\n",
    "\n",
    "\n",
    "num_attacks=100\n",
    "shot_counts = [48]\n",
    "batch_size = min(num_attacks,24)\n",
    "\n",
    "colorsim = params['scale_to_sim']\n",
    "steersim = params['scale_to_sim']\n",
    "add_at_end = params['add_at_end']\n",
    "scale_to_residnorm = params['scale_to_residnorm']\n",
    "steer_vec_type = SteeringVectorType.LEARNED if params['learn_vectors'] else params['steer_vec_type']\n",
    "coloring_vectors, steering_vectors = None,None#map_to_vectors(steer_vec_type, params['color_layers'], params['colormult'], params['steermult'], checkpoint_path, model, \"./vectors/\")\n",
    "\n",
    "checkpoint_path = model_path\n",
    "prefilled_prompts_list=[]\n",
    "zerofirst = params['zerofirst']\n",
    "SAME_ALIAS=False\n",
    "#base_model = load_model(base_model_path, base_model_path, bnb = False)\n",
    "for loop in [1,2]:#'harmful_responses_test']):#,'harmless_responses_test',,'harmless_lat_responses_test'\n",
    "    for i, test_responses_str in enumerate(['harmful_lat_responses_test','harmful_responses_test','mean_responses_test','harmful_responses_test_alias','harmful_lat_responses_test_alias','mean_responses_test_alias','harmful3_responses','harmful3_responses_alias','conv_dict_lmsys_bad','conv_dict_lmsys_good']):\n",
    "        shot_counts = [28] if 'harmful3' in test_responses_str else [20] if 'conv_dict_lmsys' in test_responses_str else [48]\n",
    "        num_attacks = 50 if lmsys else 100\n",
    "        if test_responses_str in ['parity_responses','harmless_responses_test','nice_responses_test','harmful_lat_responses_test','harmless_lat_responses_test','harmful_lat_responses_cln_test','harmless_lat_responses_cln_test','conv_dict_lmsys_bad','conv_dict_lmsys_good']:\n",
    "            alias = RoleAliasType.NONE\n",
    "            aliasstr = \"\"\n",
    "        else:\n",
    "            if params['alias'] == RoleAliasType.NONE:\n",
    "                alias = RoleAliasType.RANDOM if \"_alias\" in test_responses_str else RoleAliasType.NONE\n",
    "                aliasstr = \"_alias\" if alias == RoleAliasType.RANDOM else \"\"\n",
    "            else: \n",
    "                alias = params['alias'] if \"_alias\" in test_responses_str else RoleAliasType.NONE\n",
    "                aliasstr = \"\" if alias == RoleAliasType.NONE else \"_alias\"\n",
    "        normal_convo = True if test_responses_str in ['harmless_responses_test','nice_responses_test','harmless_lat_responses_test','harmless_lat_responses_cln_test','conv_dict_lmsys_good'] else False\n",
    "        test_responses_str=test_responses_str.replace(\"_alias\",\"\")\n",
    "        icl = False#True if test_responses_str=='parity_responses' else False\n",
    "        test_responses = globals()[test_responses_str]\n",
    "        lmsys = True if 'conv_dict_lmsys' in test_responses_str else False\n",
    "        if loop == 0:\n",
    "            suffix = \"_\" + checkpoint_path.replace(\"/checkpoint-\",\"_\").replace(\"cackerman/\",\"\") + \"_\" + str(int(SAME_ALIAS))\n",
    "            steeringtargets = [(SteeringTarget.INTERLEAVED if normal_convo else SteeringTarget.ALL_USER_PLUS_FINAL)]# ,SteeringTarget.NONE]#\n",
    "            fnames = [f'{test_responses_str}{prompts_suffix}_gen.npy',f'{test_responses_str}_alias{prompts_suffix}_gen.npy',f'{test_responses_str}_recovery{prompts_suffix}_gen.npy',f'{test_responses_str}_alias_recovery{prompts_suffix}_gen.npy']\n",
    "            use_prompts = None\n",
    "            for fname in fnames:\n",
    "                fname = nll_dir + fname\n",
    "                if os.path.exists(fname):\n",
    "                    existing_data = np.load(fname, allow_pickle=True).item()\n",
    "                    use_prompts = existing_data['prompts']\n",
    "                    break\n",
    "            if use_prompts: print(\"reusing prompts\")\n",
    "            run_numshots_generate_starttime = time.time()\n",
    "            generate = True #####if test_responses_str=='parity_responses' else False\n",
    "            prefilled_prompts = run_numshots_generate(model, test_responses, steering_vectors, coloring_vectors, zerofirst, shot_counts, num_attacks = num_attacks, alias=alias, generate = generate, steeringtargets = steeringtargets, batch_size=batch_size, use_prompts=use_prompts, colorsim=colorsim, steersim=steersim, suffix=f\"{test_responses_str}{aliasstr}_{shot_counts[0]}shot{suffix}\", add_at_end=add_at_end, lmsys=lmsys, SAME_ALIAS=SAME_ALIAS, icl=icl, scale_to_residnorm=scale_to_residnorm)\n",
    "            prefilled_prompts_list.append(prefilled_prompts)\n",
    "            print(f\"Time to finish = {time.time() - run_numshots_generate_starttime} secs\")\n",
    "            np.save(f'{test_responses_str}{aliasstr}{recoverystr}{suffix}_gen.npy', {'prompts': prefilled_prompts})\n",
    "        elif loop == 1:\n",
    "            if i==0: \n",
    "                ###del model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                #ftwoc_model = load_model(ftwoc_path, base_model_path, bnb = False)\n",
    "                ####ftwoc_model = PeftModel.from_pretrained(ftwoc_model, ftwoc_path)      \n",
    "\n",
    "                ###ftwoc_model = HookedModel(ftwoc_model)\n",
    "                \n",
    "                suffix = \"_\" + ftwoc_path.replace(\"/checkpoint-\",\"_\").replace(\"cackerman/\",\"\")\n",
    "                steeringtargets = [SteeringTarget.NONE]\n",
    "            fnames = [f'{test_responses_str}{prompts_suffix}_gen.npy',f'{test_responses_str}_alias{prompts_suffix}_gen.npy',f'{test_responses_str}_recovery{prompts_suffix}_gen.npy',f'{test_responses_str}_alias_recovery{prompts_suffix}_gen.npy']\n",
    "            use_prompts = None\n",
    "            for fname in fnames:\n",
    "                fname = nll_dir + fname\n",
    "                if os.path.exists(fname):\n",
    "                    existing_data = np.load(fname, allow_pickle=True).item()\n",
    "                    use_prompts = existing_data['prompts']\n",
    "                    break\n",
    "            if use_prompts: print(\"reusing prompts\")\n",
    "            run_numshots_generate_starttime = time.time()\n",
    "            generate = True \n",
    "            prefilled_prompts = run_numshots_generate(ftwoc_model, test_responses, steering_vectors, coloring_vectors, zerofirst, shot_counts, num_attacks = num_attacks, alias=alias, generate = generate, steeringtargets = steeringtargets, batch_size=batch_size, use_prompts=use_prompts, colorsim=colorsim, steersim=steersim, suffix=f\"{test_responses_str}{aliasstr}_{shot_counts[0]}shot{suffix}\", add_at_end=add_at_end, lmsys=lmsys, SAME_ALIAS=SAME_ALIAS, icl=icl)\n",
    "            prefilled_prompts_list.append(prefilled_prompts)\n",
    "            print(f\"Time to finish = {time.time() - run_numshots_generate_starttime} secs\")\n",
    "            np.save(f'{test_responses_str}{aliasstr}{recoverystr}{suffix}_gen.npy', {'prompts': prefilled_prompts})\n",
    "        elif loop == 2:\n",
    "            if i==0: \n",
    "                del ftwoc_model\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                #base_model = load_model(base_model_path, base_model_path, bnb = False)\n",
    "                ####base_model = HookedModel(base_model)\n",
    "                suffix = \"_untunedmodel_new\"\n",
    "                steeringtargets = [SteeringTarget.NONE]\n",
    "            fnames = [f'{test_responses_str}{prompts_suffix}_gen.npy',f'{test_responses_str}_alias{prompts_suffix}_gen.npy',f'{test_responses_str}_recovery{prompts_suffix}_gen.npy',f'{test_responses_str}_alias_recovery{prompts_suffix}_gen.npy']\n",
    "            use_prompts = None\n",
    "            for fname in fnames:\n",
    "                fname = nll_dir + fname\n",
    "                if os.path.exists(fname):\n",
    "                    existing_data = np.load(fname, allow_pickle=True).item()\n",
    "                    use_prompts = existing_data['prompts']\n",
    "                    break\n",
    "            if use_prompts: print(\"reusing prompts\")\n",
    "            run_numshots_generate_starttime = time.time()\n",
    "            generate = True #####if test_responses_str=='parity_responses' else False\n",
    "            prefilled_prompts = run_numshots_generate(base_model, test_responses, steering_vectors, coloring_vectors, zerofirst, shot_counts, num_attacks = num_attacks, alias=alias, generate = generate, steeringtargets = steeringtargets, batch_size=batch_size, use_prompts=use_prompts, colorsim=colorsim, steersim=steersim, suffix=f\"{test_responses_str}{aliasstr}_{shot_counts[0]}shot{suffix}\", add_at_end=add_at_end, lmsys=lmsys, SAME_ALIAS=SAME_ALIAS, icl=icl)\n",
    "            print(f\"Time to finish = {time.time() - run_numshots_generate_starttime} secs\")\n",
    "            np.save(f'{test_responses_str}{aliasstr}{recoverystr}{suffix}_gen.npy', {'prompts': prefilled_prompts})\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
