{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9954bfa7714b8da94f405e4c41a0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Installs/imports\n",
    "#!pip install torch transformers datasets tabulate scikit-learn seaborn accelerate bitsandbytes\n",
    "from initialize import *\n",
    "from enhanced_hooking import get_activations, add_activations_and_generate, clear_hooks, get_activations_and_generate, zeroout_projections_and_generate\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "class SteeringType(Enum):\n",
    "    IN_PROMPT = \"In prompt\"\n",
    "    CONTINUOUS = \"Continuous\"\n",
    "class AggType(Enum):\n",
    "    MEANDIFF = \"MeanDiff\"\n",
    "    PCA = \"PCA\"\n",
    "\n",
    "### Load the model\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "base_model_path: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "model_path=base_model_path\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "_ = torch.set_grad_enabled(False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, token=HF_TOKEN, quantization_config=bnb_config, device_map=\"auto\")\n",
    "device = model.device\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, token=HF_TOKEN)\n",
    "model.tokenizer = tokenizer\n",
    "if model.tokenizer.pad_token is None:\n",
    "    new_pad_token = model.tokenizer.eos_token\n",
    "    num_added_tokens = model.tokenizer.add_special_tokens({'pad_token': new_pad_token})\n",
    "    model.resize_token_embeddings(len(model.tokenizer))\n",
    "    model.config.pad_token_id = model.tokenizer.pad_token_id\n",
    "model_numlayers = model.config.num_hidden_layers\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "from data import *\n",
    "\n",
    "sad_responses, _ = load_data_sad(ddir = \"completions_full\")\n",
    "sad_articles = load_from_json(f\"starts_full/starts_train.json\")\n",
    "prompts = []\n",
    "for key in sad_responses['llama3_8bchat'].keys():\n",
    "    self_summary = sad_responses['llama3_8bchat'][key].replace(\"\\n\\n\",\"\\n\").strip()\n",
    "    other_summary = sad_responses['human'][key].replace(\"\\n\\n\",\"\\n\").strip()\n",
    "    article = next(d['text'] for d in sad_articles if d['id'] == key).replace(\"\\n\\n\",\"\\n\").strip()\n",
    "        \n",
    "    prompts.append(self_summary)\n",
    "    prompts.append(other_summary)\n",
    "    prompts.append(article)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [28:34<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "### Run prompts through model and capture activations, averaging over positions of interest\n",
    "\n",
    "model.tokenizer.padding_side = \"right\"\n",
    "layers = ['embed'] + list(range(0,31))\n",
    "get_at='end'\n",
    "clear_hooks(model)\n",
    "\n",
    "prompts_samp = random.sample(prompts, 3000)\n",
    "\n",
    "accumulated_activations = defaultdict(lambda: defaultdict(lambda: torch.empty(0)))\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "for i in tqdm(range(0, len(prompts_samp), batch_size)):\n",
    "    batch = prompts_samp[i:i+batch_size]\n",
    "    encoded_pos = model.tokenizer(batch, return_tensors=\"pt\", padding=True)\n",
    "    batch_tokens = encoded_pos['input_ids']\n",
    "\n",
    "    layers_positions = {layer: [] for layer in layers}\n",
    "\n",
    "    for input_ids in batch_tokens:\n",
    "        positions = list(range(0,len(input_ids)))\n",
    "        for layer in layers:\n",
    "            layers_positions[layer].append(positions)\n",
    "\n",
    "    activations = get_activations(model, batch_tokens, layers_positions, get_at=get_at)\n",
    "    mean_activations = {} \n",
    "    \n",
    "    for layer, positions in activations.items():\n",
    "        batch_size = next(iter(positions.values())).shape[0] # Get batch size from any position tensor\n",
    "        layer_sum = torch.zeros(batch_size, positions[next(iter(positions.keys()))].shape[-1]) # Initialize tensor with zeros\n",
    "        \n",
    "        num_positions = len(positions) \n",
    "        for pos, tensor in positions.items():\n",
    "            layer_sum += tensor\n",
    "        \n",
    "        mean_activations[layer] = layer_sum / num_positions\n",
    "\n",
    "    for layer, tensor in mean_activations.items():\n",
    "        accumulated_activations[layer][0] = torch.cat([accumulated_activations[layer][0], tensor], dim=0)\n",
    "    \n",
    "    del activations, mean_activations\n",
    "    torch.cuda.empty_cache()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=accumulated_activations['embed'][0]\n",
    "\n",
    "num_pcs = 400 #100\n",
    "pca = PCA(n_components=num_pcs)\n",
    "pca.fit(X)\n",
    "principal_components = pca.components_  # shape: (num_pcs, d_embed)\n",
    "\n",
    "# Define a function to orthogonalize a vector against a set of directions.\n",
    "def orthogonalize(vec, directions):\n",
    "    # directions: array of shape (num_dirs, d_embed)\n",
    "    # vec: array of shape (d_embed,)\n",
    "    for d in directions:\n",
    "        proj = np.dot(vec, d) * d\n",
    "        vec = vec - proj\n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > 0:\n",
    "        vec = vec / norm\n",
    "    return vec\n",
    "\n",
    "# Create tag vectors.\n",
    "# Initialize random vectors and orthogonalize them against the principal components.\n",
    "d_embed = X.shape[1]\n",
    "tag_vec_user = np.random.randn(d_embed)\n",
    "tag_vec_assistant = np.random.randn(d_embed)\n",
    "\n",
    "tag_vec_user = orthogonalize(tag_vec_user, principal_components)\n",
    "tag_vec_assistant = orthogonalize(tag_vec_assistant, principal_components)\n",
    "\n",
    "# Ensure tag_vec_assistant is also orthogonal to tag_vec_user.\n",
    "proj = np.dot(tag_vec_assistant, tag_vec_user) * tag_vec_user\n",
    "tag_vec_assistant = tag_vec_assistant - proj\n",
    "norm = np.linalg.norm(tag_vec_assistant)\n",
    "if norm > 0:\n",
    "    tag_vec_assistant = tag_vec_assistant / norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.8157915708197409)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ofname = 'steering_vectors_orth_asst_embed.pkl'\n",
    "with open(ofname, 'wb') as f:\n",
    "    pickle.dump(torch.tensor(tag_vec_assistant), f)\n",
    "ofname = 'steering_vectors_orth_user_embed.pkl'\n",
    "with open(ofname, 'wb') as f:\n",
    "    pickle.dump(torch.tensor(tag_vec_user), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer=embed, num_pcs=400, var=0.8157953273569934\n",
      "layer=0, num_pcs=100, var=0.8463752253998517\n",
      "layer=1, num_pcs=8, var=0.9987105877217431\n",
      "layer=2, num_pcs=12, var=0.9971450308642328\n",
      "layer=3, num_pcs=16, var=0.9920824234842771\n",
      "layer=4, num_pcs=20, var=0.9847806068412651\n",
      "layer=5, num_pcs=24, var=0.978013067226529\n",
      "layer=6, num_pcs=28, var=0.9712825342572504\n",
      "layer=7, num_pcs=32, var=0.9644278916486126\n",
      "layer=8, num_pcs=36, var=0.9568542732076477\n",
      "layer=9, num_pcs=40, var=0.949198062014635\n",
      "layer=10, num_pcs=44, var=0.9486434641403036\n",
      "layer=11, num_pcs=48, var=0.9506181545360557\n",
      "layer=12, num_pcs=52, var=0.947403507465094\n",
      "layer=13, num_pcs=56, var=0.9314517096345616\n",
      "layer=14, num_pcs=60, var=0.9252147199796454\n",
      "layer=15, num_pcs=64, var=0.8983186799583137\n",
      "layer=16, num_pcs=68, var=0.8815866028851663\n",
      "layer=17, num_pcs=72, var=0.8552371468024186\n",
      "layer=18, num_pcs=76, var=0.8433537917738927\n",
      "layer=19, num_pcs=80, var=0.8360048791918033\n",
      "layer=20, num_pcs=84, var=0.8253638284349869\n",
      "layer=21, num_pcs=88, var=0.8132542724490792\n",
      "layer=22, num_pcs=92, var=0.7957557463451362\n",
      "layer=23, num_pcs=96, var=0.787685478490532\n",
      "layer=24, num_pcs=100, var=0.7783989015273706\n",
      "layer=25, num_pcs=104, var=0.7752758647779345\n",
      "layer=26, num_pcs=108, var=0.7696576142130731\n",
      "layer=27, num_pcs=112, var=0.7679256793795187\n",
      "layer=28, num_pcs=116, var=0.7575840092944178\n",
      "layer=29, num_pcs=120, var=0.7633285103633629\n",
      "layer=30, num_pcs=124, var=0.7598961696583909\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "layerwise_vectors = {}\n",
    "\n",
    "for layer in accumulated_activations.keys():\n",
    "    X = accumulated_activations[layer][0]\n",
    "    \n",
    "    num_pcs = 400 if layer == 'embed' else 100 if int(layer) == 0 else 4+4*int(layer)\n",
    "    pca = PCA(n_components=num_pcs)\n",
    "    pca.fit(X)\n",
    "    principal_components = pca.components_  # shape: (num_pcs, d_embed)\n",
    "    \n",
    "    # Define a function to orthogonalize a vector against a set of directions.\n",
    "    def orthogonalize(vec, directions):\n",
    "        for d in directions:\n",
    "            proj = np.dot(vec, d) * d\n",
    "            vec = vec - proj\n",
    "        norm = np.linalg.norm(vec)\n",
    "        if norm > 0:\n",
    "            vec = vec / norm\n",
    "        return vec\n",
    "\n",
    "    # Create tag vectors.\n",
    "    d_embed = X.shape[1]\n",
    "    tag_vec_user = np.random.randn(d_embed)\n",
    "    tag_vec_assistant = np.random.randn(d_embed)\n",
    "    \n",
    "    tag_vec_user = orthogonalize(tag_vec_user, principal_components)\n",
    "    tag_vec_assistant = orthogonalize(tag_vec_assistant, principal_components)\n",
    "    \n",
    "    # Ensure tag_vec_assistant is also orthogonal to tag_vec_user.\n",
    "    proj = np.dot(tag_vec_assistant, tag_vec_user) * tag_vec_user\n",
    "    tag_vec_assistant = tag_vec_assistant - proj\n",
    "    norm = np.linalg.norm(tag_vec_assistant)\n",
    "    if norm > 0:\n",
    "        tag_vec_assistant = tag_vec_assistant / norm\n",
    "    \n",
    "    layerwise_vectors[layer] = {\n",
    "        'user': torch.tensor(tag_vec_user),\n",
    "        'assistant': torch.tensor(tag_vec_assistant)\n",
    "    }\n",
    "    print(f\"layer={layer}, num_pcs={num_pcs}, var={np.sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "# Save layerwise vectors to files.\n",
    "with open('steering_vectors_orth_user_all.pkl', 'wb') as f:\n",
    "    pickle.dump({layer: layerwise_vectors[layer]['user'] for layer in layerwise_vectors}, f)\n",
    "with open('steering_vectors_orth_asst_all.pkl', 'wb') as f:\n",
    "    pickle.dump({layer: layerwise_vectors[layer]['assistant'] for layer in layerwise_vectors}, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
